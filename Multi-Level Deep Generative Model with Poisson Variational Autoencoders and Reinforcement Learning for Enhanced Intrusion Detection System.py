# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u4kyLuvEZdXbjEeb432kvAXLgNIJNAn4
"""

import torch
print("PyTorch:", torch.__version__)
print("CUDA available?", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
    x = torch.randn(8192, 8192, device="cuda"); print("Tensor on:", x.device)
else:
    print("❌ still CPU — check driver and kernel selection")

import sys
print(sys.executable)

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade pip
# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
# %pip install pandas numpy scikit-learn matplotlib xgboost joblib
# %pip install "stable-baselines3[extra]==2.3.2" gymnasium==0.29.1
# %pip install catboost

import torch, pandas as pd, numpy as np, xgboost, sklearn
print("PyTorch:", torch.__version__)
print("CUDA available?", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
print("pandas:", pd.__version__)
print("numpy:", np.__version__)
print("sklearn:", sklearn.__version__)
print("xgboost:", xgboost.__version__)

# Commented out IPython magic to ensure Python compatibility.
# BLOCK 1 —
import os, numpy as np, pandas as pd, gc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
# %pip install -q "gymnasium>=0.29.0" "stable-baselines3[extra]>=2.3.0"

CSV_PATH = "NF-UQ-NIDS-v2.csv"
SEED = 42
BENIGN_LIMIT = 500_000
ATTACK_MAX_SAMPLES = 100_000
CHUNK_SIZE = 500_000

rng = np.random.default_rng(SEED)
os.makedirs("artifacts", exist_ok=True)

# === canonical groups for label consolidation ===
recon_group = {"Reconnaissance", "Analysis", "Fuzzers", "scanning"}
malware_group = {"ransomware", "Worms", "Shellcode", "Backdoor", "Exploits"}

def normalize_attack(x: str) -> str:
    x = str(x).strip()
    if x in recon_group: return "Reconnaissance"
    if x in malware_group: return "malware"
    return x

# ===1: collect row indices to keep ===
idx_benign, idx_attack_per_type, offset = [], {}, 0
usecols = None
for chunk in pd.read_csv(CSV_PATH, chunksize=CHUNK_SIZE, low_memory=False):
    if usecols is None: usecols = list(chunk.columns)
    chunk = chunk.dropna(subset=['Attack'])
    chunk['Attack'] = chunk['Attack'].apply(normalize_attack)
    labels  = chunk['Label'].astype(int).values
    attacks = chunk['Attack'].values
    for i, (lbl, atk) in enumerate(zip(labels, attacks)):
        abs_idx = offset + i
        if lbl == 0: idx_benign.append(abs_idx)
        else: idx_attack_per_type.setdefault(atk, []).append(abs_idx)
    offset += len(chunk)

print(f"\n✅ Collected:")
print(f"  Benign samples: {len(idx_benign):,}")
for atk, idxs in idx_attack_per_type.items():
    print(f"  Attack '{atk:20s}': {len(idxs):,}")

# sample benign + per-class attacks
sel_benign = rng.choice(idx_benign, size=min(BENIGN_LIMIT, len(idx_benign)), replace=False)
sel_attack, final_attack_classes = [], {}
for atk, idxs in idx_attack_per_type.items():
    take = min(len(idxs), ATTACK_MAX_SAMPLES)
    sampled = rng.choice(idxs, size=take, replace=False)
    sel_attack.extend(sampled)
    final_attack_classes[atk] = len(sampled)

print(f"\n🎯 Final attack class sampling:")
for atk, count in final_attack_classes.items():
    print(f"  {atk:20s} → {count:6,} samples")

selected_lines = set((i + 1) for i in np.concatenate([sel_benign, sel_attack]))
def skip_line(i): return i != 0 and i not in selected_lines

# === PASS 2: load only selected rows ===
print("\n📦 Second pass: loading selected rows only...")
df = pd.read_csv(CSV_PATH, skiprows=skip_line, low_memory=False)
print("✅ Loaded shape:", df.shape)

# === cleaning (labels only) ===
df = df.dropna(subset=['Attack']).reset_index(drop=True)
df['Attack'] = df['Attack'].apply(normalize_attack)

# target encoding
le_attack = LabelEncoder()
df['Attack_Label'] = le_attack.fit_transform(df['Attack'])
print("\n Multiclass Attack Label Mapping:")
for name, code in zip(le_attack.classes_, le_attack.transform(le_attack.classes_)):
    print(f"  {code:2d} → {name}")

# drop obvious leakage / IDs (keep ports for rules/analysis)
drop_cols = ['DNS_QUERY_ID', 'FTP_COMMAND_RET_CODE']
df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors='ignore')

# identify dtypes (no global dropna; we’ll impute numerics later)
num_cols_all = df.select_dtypes(include=['number']).columns.tolist()
if 'Attack_Label' in num_cols_all: num_cols_all.remove('Attack_Label')
cat_cols_all = [c for c in df.columns if c not in num_cols_all and c not in ['Attack','Label','Attack_Label']]

print(f"\nFeature summary:\n  Numeric: {len(num_cols_all)} | Categorical: {len(cat_cols_all)}")

# split (stratified)
y = df['Attack_Label'].astype(int)
X = df.drop(columns=['Attack','Label','Attack_Label'], errors='ignore')

# optional: drop IPs to avoid leakage unless you group-split later
leak_cols = ['IPV4_SRC_ADDR','IPV4_DST_ADDR']
X = X.drop(columns=[c for c in leak_cols if c in X.columns], errors='ignore')

# ensure lists reflect drops
num_cols_all = [c for c in num_cols_all if c in X.columns]
cat_cols_all = [c for c in cat_cols_all if c in X.columns]

X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.15, stratify=y, random_state=SEED
)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.15, stratify=y_trainval, random_state=SEED
)

# === categorical encoders on TRAIN only ===
label_encoders = {}
for col in cat_cols_all:
    le = LabelEncoder()
    le.fit(X_train[col].astype(str))
    label_encoders[col] = le

def transform_cats(df_part):
    if len(cat_cols_all) == 0:
        return np.zeros((len(df_part),0), dtype=np.int64)
    out = []
    for col, le in label_encoders.items():
        ser = df_part[col].astype(str)
        mapping = {k:i for i,k in enumerate(le.classes_)}
        out.append(ser.map(mapping).fillna(len(le.classes_)).astype(int).to_numpy())
    return np.column_stack(out).astype(np.int64)

Xcat_train = transform_cats(X_train)
Xcat_val   = transform_cats(X_val)
Xcat_test  = transform_cats(X_test)

# === ROBUST NUMERIC PIPELINE (prevents overflow/NaNs) ===
# work in float64 during cleaning
Xnum_train_df = X_train[num_cols_all].astype('float64').copy()
Xnum_val_df   = X_val[num_cols_all].astype('float64').copy()
Xnum_test_df  = X_test[num_cols_all].astype('float64').copy()

# replace inf/-inf with NaN, then impute NaN with TRAIN medians
for dfp in (Xnum_train_df, Xnum_val_df, Xnum_test_df):
    dfp.replace([np.inf, -np.inf], np.nan, inplace=True)
medians = Xnum_train_df.median()
for dfp in (Xnum_train_df, Xnum_val_df, Xnum_test_df):
    dfp.fillna(medians, inplace=True)

q_low  = Xnum_train_df.quantile(0.001)
q_high = Xnum_train_df.quantile(0.999)
eps = 1e-9
q_high = np.maximum(q_high, q_low + eps)  # avoid identical bounds
for dfp in (Xnum_train_df, Xnum_val_df, Xnum_test_df):
    dfp.clip(lower=q_low, upper=q_high, axis=1, inplace=True)

# per-feature Poisson scales from PRE-scaled train distribution
q95_pre = Xnum_train_df.quantile(0.95)
poisson_scales = (50.0 / (q95_pre.values + 1e-6)).astype(np.float32)  # shape [Dnum]

# MinMax scaling (fit on TRAIN only), then cast to float32
scaler = MinMaxScaler()
Xnum_train = scaler.fit_transform(Xnum_train_df.to_numpy(dtype=np.float64)).astype(np.float32)
Xnum_val   = scaler.transform(Xnum_val_df.to_numpy(dtype=np.float64)).astype(np.float32)
Xnum_test  = scaler.transform(Xnum_test_df.to_numpy(dtype=np.float64)).astype(np.float32)

# sanity: all finite
for arr, tag in [(Xnum_train,"train"), (Xnum_val,"val"), (Xnum_test,"test")]:
    if not np.isfinite(arr).all():
        bad = np.where(~np.isfinite(arr))
        raise RuntimeError(f"[{tag}] non-finite after scaling at rows {bad[0][:5]}, cols {bad[1][:5]}")

# correlated feature drop on TRAIN only, then align others & scales
corr = pd.DataFrame(Xnum_train, columns=num_cols_all).corr().abs()
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
to_drop = [col for col in upper.columns if any(upper[col] >= 0.95)]
keep_cols = [c for c in num_cols_all if c not in to_drop]
if to_drop:
    idx_keep = [num_cols_all.index(c) for c in keep_cols]
    Xnum_train = Xnum_train[:, idx_keep]
    Xnum_val   = Xnum_val[:,   idx_keep]
    Xnum_test  = Xnum_test[:,  idx_keep]
    num_cols_all   = keep_cols
    poisson_scales = poisson_scales[idx_keep]


def _lc_names(cols):
    return {c: c.lower() for c in cols}

lc = _lc_names(num_cols_all)

def pick_by_keywords(candidates, *keyword_groups):
    """
    Select columns whose lowercase name contains ALL tokens of at least one keyword_group.
    Example: pick_by_keywords(num_cols_all, ('in_pkts',), ('out_pkts', 'bytes'))
    """
    chosen = set()
    for group in keyword_groups:
        toks = tuple(t.lower() for t in group)
        for orig, low in lc.items():
            if all(tok in low for tok in toks):
                chosen.add(orig)
    return [c for c in num_cols_all if c in chosen]  # preserve original order

# Level 1 (packet-ish, fine-grain size/flags/min/max)
l1_cols = []
l1_cols += pick_by_keywords(num_cols_all,
    ('num_pkts_64',), ('num_pkts_65',), ('num_pkts_128',), ('num_pkts_256',),
    ('num_pkts_512',), ('num_pkts_1024',), ('num_pkts_1514',),
    ('shortest_flow_pkt',), ('longest_flow_pkt',),
    ('min_ip_pkt_len',), ('max_ip_pkt_len',),
    ('icmp_type',), ('tcp_flags',)
)

# Level 2 (flow-ish, core counts/bytes/durations/retransmits)
l2_cols = []
l2_cols += pick_by_keywords(num_cols_all,
    ('in_pkts',), ('out_pkts',), ('in_bytes',), ('out_bytes',),
    ('retransmitted',), ('flow_duration',), ('duration_in',), ('duration_out',),
    ('bpp',), ('pkts', 'bytes')
)

# Level 3 (session/context-ish, rates/throughput/directionality)
l3_cols = []
l3_cols += pick_by_keywords(num_cols_all,
    ('avg_throughput',), ('second_bytes',),
    ('dst_to_src',), ('src_to_dst',), ('throughput',),
    ('flow_duration_milliseconds',), ('duration', 'millisecond')
)

# --- de-dup & ensure non-empty, backfill deterministically to keep sets distinct ---
def backfill(target, avoid, k_min):
    remaining = [c for c in num_cols_all if (c not in target) and (c not in avoid)]
    need = max(0, k_min - len(target))
    target += remaining[:need]

# Make them reasonably sized; tune if you want.
backfill(l1_cols, set(),  max(10, min(20, len(num_cols_all)//5)))
backfill(l2_cols, set(l1_cols), max(12, min(24, len(num_cols_all)//4)))
backfill(l3_cols, set(l1_cols) | set(l2_cols), max(12, min(24, len(num_cols_all)//4)))

# Convert to indices versus the *current* Xnum_* arrays (already scaled & post-drop)
idx_l1 = np.array([num_cols_all.index(c) for c in l1_cols], dtype=np.int64)
idx_l2 = np.array([num_cols_all.index(c) for c in l2_cols], dtype=np.int64)
idx_l3 = np.array([num_cols_all.index(c) for c in l3_cols], dtype=np.int64)

# Slice per-level numeric tensors
Xnum_train_l1 = Xnum_train[:, idx_l1]; Xnum_val_l1 = Xnum_val[:, idx_l1]; Xnum_test_l1 = Xnum_test[:, idx_l1]
Xnum_train_l2 = Xnum_train[:, idx_l2]; Xnum_val_l2 = Xnum_val[:, idx_l2]; Xnum_test_l2 = Xnum_test[:, idx_l2]
Xnum_train_l3 = Xnum_train[:, idx_l3]; Xnum_val_l3 = Xnum_val[:, idx_l3]; Xnum_test_l3 = Xnum_test[:, idx_l3]

# Per-level Poisson scales (subset from global; still useful as generic numeric scaling inside the PVAE Poisson head)
poisson_scales_l1 = poisson_scales[idx_l1]
poisson_scales_l2 = poisson_scales[idx_l2]
poisson_scales_l3 = poisson_scales[idx_l3]

# Persist per-level arrays and metadata (we keep your original saves too)
np.save('Xnum_l1_train_sir.npy', Xnum_train_l1)
np.save('Xnum_l1_val_sir.npy',   Xnum_val_l1)
np.save('Xnum_l1_test_sir.npy',  Xnum_test_l1)

np.save('Xnum_l2_train_sir.npy', Xnum_train_l2)
np.save('Xnum_l2_val_sir.npy',   Xnum_val_l2)
np.save('Xnum_l2_test_sir.npy',  Xnum_test_l2)

np.save('Xnum_l3_train_sir.npy', Xnum_train_l3)
np.save('Xnum_l3_val_sir.npy',   Xnum_val_l3)
np.save('Xnum_l3_test_sir.npy',  Xnum_test_l3)

pd.to_pickle(l1_cols, "artifacts/num_cols_l1.pkl")
pd.to_pickle(l2_cols, "artifacts/num_cols_l2.pkl")
pd.to_pickle(l3_cols, "artifacts/num_cols_l3.pkl")

np.save("artifacts/poisson_scales_l1.npy", poisson_scales_l1.astype(np.float32))
np.save("artifacts/poisson_scales_l2.npy", poisson_scales_l2.astype(np.float32))
np.save("artifacts/poisson_scales_l3.npy", poisson_scales_l3.astype(np.float32))

print(f"[Multi-level] L1 dims={Xnum_train_l1.shape[1]}, L2 dims={Xnum_train_l2.shape[1]}, L3 dims={Xnum_train_l3.shape[1]}")

# === persist everything ===
np.save('Xnum_train_sir.npy', Xnum_train)
np.save('Xnum_val_sir.npy',   Xnum_val)
np.save('Xnum_test_sir.npy',  Xnum_test)
np.save('Xcat_train_sir.npy', Xcat_train)
np.save('Xcat_val_sir.npy',   Xcat_val)
np.save('Xcat_test_sir.npy',  Xcat_test)
np.save('y_train_sir.npy',    y_train.to_numpy())
np.save('y_val_sir.npy',      y_val.to_numpy())
np.save('y_test_sir.npy',     y_test.to_numpy())

pd.to_pickle(num_cols_all, "artifacts/num_cols.pkl")
pd.to_pickle(cat_cols_all, "artifacts/cat_cols.pkl")
pd.to_pickle(label_encoders, "artifacts/label_encoders.pkl")
pd.to_pickle(scaler, "artifacts/minmax_scaler.pkl")
np.save("artifacts/poisson_scales.npy", poisson_scales.astype(np.float32))
np.save("keep_cols_sir.npy", np.array(num_cols_all, dtype=object))

print(f"\n✅ Data split + robust numeric preprocessing done.")
print(f"  Xnum_train: {Xnum_train.shape} | Xcat_train: {Xcat_train.shape}")

# BLOCK 2 — DataLoaders
import os, numpy as np, torch
from torch.utils.data import DataLoader, TensorDataset

torch.backends.cudnn.benchmark = True
try:
    torch.set_float32_matmul_precision('high')
except Exception:
    pass

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# load
Xnum_train = np.load('Xnum_train_sir.npy', mmap_mode='r')
Xnum_val   = np.load('Xnum_val_sir.npy',   mmap_mode='r')
Xcat_train = np.load('Xcat_train_sir.npy', mmap_mode='r')
Xcat_val   = np.load('Xcat_val_sir.npy',   mmap_mode='r')
y_train    = np.load('y_train_sir.npy',    mmap_mode='r')
y_val      = np.load('y_val_sir.npy',      mmap_mode='r')

# --- [NEW] Load per-level numerics ---
Xnum1_train = np.load('Xnum_l1_train_sir.npy', mmap_mode='r')
Xnum2_train = np.load('Xnum_l2_train_sir.npy', mmap_mode='r')
Xnum3_train = np.load('Xnum_l3_train_sir.npy', mmap_mode='r')

Xnum1_val   = np.load('Xnum_l1_val_sir.npy',   mmap_mode='r')
Xnum2_val   = np.load('Xnum_l2_val_sir.npy',   mmap_mode='r')
Xnum3_val   = np.load('Xnum_l3_val_sir.npy',   mmap_mode='r')

# torch tensors (explicit arrays to avoid memmap stride issues)
train_ds = TensorDataset(
    torch.from_numpy(np.array(Xnum1_train)).float(),
    torch.from_numpy(np.array(Xnum2_train)).float(),
    torch.from_numpy(np.array(Xnum3_train)).float(),
    torch.from_numpy(np.array(Xcat_train)).long(),
    torch.from_numpy(np.array(y_train)).long()
)
val_ds = TensorDataset(
    torch.from_numpy(np.array(Xnum1_val)).float(),
    torch.from_numpy(np.array(Xnum2_val)).float(),
    torch.from_numpy(np.array(Xnum3_val)).float(),
    torch.from_numpy(np.array(Xcat_val)).long(),
    torch.from_numpy(np.array(y_val)).long()
)

BATCH = 2048
NUM_WORKERS = max(4, min(12, os.cpu_count() - 2))

DL_train = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  drop_last=False, num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)
DL_val   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, drop_last=False,
                      num_workers=NUM_WORKERS, pin_memory=True, persistent_workers=True, prefetch_factor=4)

n_classes = int(np.max(y_train) + 1)
input_num_dim_l1 = Xnum1_train.shape[1]
input_num_dim_l2 = Xnum2_train.shape[1]
input_num_dim_l3 = Xnum3_train.shape[1]

# class cardinalities for cat embeddings (same as before)
cat_cardinalities = []
if os.path.exists("artifacts/cat_cols.pkl"):
    import pandas as pd
    cat_cols_all = pd.read_pickle("artifacts/cat_cols.pkl")
    le_map = pd.read_pickle("artifacts/label_encoders.pkl")
    for c in cat_cols_all:
        cat_cardinalities.append(len(le_map[c].classes_) + 1)
else:
    cat_cardinalities = [0]

# per-level poisson scales (buffers inside the PVAE levels)
ps1 = torch.from_numpy(np.load("artifacts/poisson_scales_l1.npy")).to(device)
ps2 = torch.from_numpy(np.load("artifacts/poisson_scales_l2.npy")).to(device)
ps3 = torch.from_numpy(np.load("artifacts/poisson_scales_l3.npy")).to(device)

n_classes = int(np.max(y_train) + 1)
input_num_dim = Xnum_train.shape[1]
cat_cardinalities = []
if os.path.exists("artifacts/cat_cols.pkl"):
    import pandas as pd
    cat_cols_all = pd.read_pickle("artifacts/cat_cols.pkl")
    le_map = pd.read_pickle("artifacts/label_encoders.pkl")
    for c in cat_cols_all:
        cat_cardinalities.append(len(le_map[c].classes_) + 1)
else:
    cat_cardinalities = [0]

poisson_scales = torch.from_numpy(np.load("artifacts/poisson_scales.npy")).to(device)

# BLOCK 3 — Model: PVAEs + flows + stable radial recalib + AMP + schedules
import math, numpy as np, torch, torch.nn as nn, torch.nn.functional as F
from contextlib import nullcontext
from math import pi

# ---------- flows ----------
class PlanarFlow(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.u = nn.Parameter(torch.randn(dim) * 0.01)
        self.w = nn.Parameter(torch.randn(dim) * 0.01)
        self.b = nn.Parameter(torch.zeros(()))
    def _u_hat(self):
        wu = torch.dot(self.w, self.u)
        m = -1.0 + F.softplus(wu)
        return self.u + (m - wu) * self.w / (self.w.norm() ** 2 + 1e-8)
    def forward(self, z):
        w, u, b = self.w, self._u_hat(), self.b
        lin = z @ w + b
        h = torch.tanh(lin)
        z_new = z + u * h.unsqueeze(-1)
        psi = (1 - torch.tanh(lin) ** 2).unsqueeze(-1) * w
        logdet = torch.log(torch.abs(1 + (psi @ u.unsqueeze(-1)).squeeze(-1)) + 1e-8)
        return z_new, logdet

class AffineCoupling(nn.Module):
    def __init__(self, dim, hidden=256, context_dim=128, mask_even=True):
        super().__init__()
        self.dim = dim; self.mask_even = mask_even
        self.net = nn.Sequential(
            nn.Linear(dim//2 + context_dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, dim)
        )
    def forward(self, z, context):
        B, D = z.shape
        idx = torch.arange(D, device=z.device)
        mask = (idx % 2 == 0) if self.mask_even else (idx % 2 == 1)
        z1 = z[:, mask]; z2 = z[:, ~mask]
        h = torch.cat([z1, context], dim=1)
        out = self.net(h); t, s = out[:, :z2.shape[1]], out[:, z2.shape[1]:]
        s = torch.tanh(s)
        z2_new = z2 * torch.exp(s) + t
        z_new = torch.empty_like(z); z_new[:, mask] = z1; z_new[:, ~mask] = z2_new
        logdet = s.sum(dim=1)
        return z_new, logdet

class MADE(nn.Module):
    def __init__(self, dim, hidden=256):
        super().__init__()
        self.fc1 = nn.Linear(dim, hidden); self.fc2 = nn.Linear(hidden, hidden); self.fc3 = nn.Linear(hidden, dim*2)
    def forward(self, x):
        h = F.relu(self.fc1(x)); h = F.relu(self.fc2(h)); out = self.fc3(h)
        m, a = out.chunk(2, dim=1); a = torch.tanh(a); return m, a

class MAF(nn.Module):
    def __init__(self, dim): super().__init__(); self.net = MADE(dim)
    def forward(self, z):
        m, a = self.net(z); z_new = (z - m) * torch.exp(-a); logdet = -a.sum(dim=1); return z_new, logdet

# ----------latent recalibrator ----------
class LRQuantileRadialCalibrator(nn.Module):
    """
    Post-hoc, explicit radial recalibration:
      - Fit on a calibration split: build a piecewise-linear, monotone mapping r(l) so that
        U = F_L(l) ≈ Uniform(0,1) and r(l) = Q_base(U), where Q_base is the empirical
        quantile function of ||Z|| with Z~N(0,I) in R^D.
      - Forward: z' = (r/ l) * z,  log|det J| = (D-1)*log(r/l) + log(dr/dl).
    Notes:
      * During TRAINING (fitted=False), this is the identity map (no effect, zero logdet).
      * After .fit(...), it becomes a fixed, differentiable piecewise-linear warp.
      * Supports class-conditional calibration if n_classes is provided.
    """
    def __init__(self, dim, n_classes=None, n_knots=256, eps=1e-8):
        super().__init__()
        self.dim = dim
        self.n_classes = n_classes
        self.n_knots = n_knots
        self.eps = eps
        # buffers holding knot locations for l (input) and r (target) per class
        # shape: [C, K] where C = n_classes or 1
        C = n_classes if (n_classes is not None and n_classes > 0) else 1
        self.register_buffer("knots_l", torch.zeros(C, n_knots))   # filled after fit()
        self.register_buffer("knots_r", torch.zeros(C, n_knots))   # filled after fit()
        self.fitted = False

    @torch.no_grad()
    def fit(self, z_calib, y_calib=None, base_samples=200_000, device=None):
        """
        z_calib: Tensor [N, D]  (top-level PRE-RECALIB z i.e., zK) from a calibration split
        y_calib: Tensor [N] with class indices (optional; class-conditional if provided)
        """
        # pick a single device for everything: prefer the module's buffer device
        dev = device or self.knots_l.device
        z_calib = z_calib.to(dev)
        l = torch.norm(z_calib, dim=1)  # latent norms

        # Base quantiles for ||Z|| under N(0,I): empirical, stable, library-free
        with torch.random.fork_rng(devices=[]):
            torch.manual_seed(12345)
            z_base = torch.randn(base_samples, self.dim, device=dev)
        base_norms = z_base.norm(dim=1).sort().values
        u_grid = torch.linspace(0.001, 0.999, self.n_knots, device=dev)
        idx_base = torch.clamp((u_grid * (base_norms.numel() - 1)).round().long(), 0, base_norms.numel()-1)
        r_knots_global = base_norms[idx_base]  # [K]

        # class buckets
        if self.n_classes is not None and y_calib is not None:
            C = self.n_classes
            y = y_calib.to(dev).long()
            classes = torch.arange(C, device=dev)
        else:
            C = 1
            y = None
            classes = torch.tensor([0], device=dev)

        K = self.n_knots
        new_knots_l = torch.zeros(C, K, device=dev)
        new_knots_r = torch.zeros(C, K, device=dev)

        for c in classes:
            mask = (y == c) if y is not None else torch.ones_like(l, dtype=torch.bool)
            lc = l[mask]
            # fallback to global if too few samples for that class
            if lc.numel() < 100:
                lc = l
            lc_sorted = lc.sort().values
            idx_l = torch.clamp((u_grid * (lc_sorted.numel() - 1)).round().long(), 0, lc_sorted.numel()-1)
            l_knots = lc_sorted[idx_l]  # [K]

            # --- enforce strict monotonicity, safely (O(K) loop is fine for K=256) ---
            l_knots = torch.clamp(l_knots, min=0.0)
            for i in range(1, K):
                # ensure l_knots[i] >= l_knots[i-1] + eps
                if l_knots[i] <= l_knots[i-1]:
                    l_knots[i] = l_knots[i-1] + 1e-6

            new_knots_l[c if y is not None else 0] = l_knots
            new_knots_r[c if y is not None else 0] = r_knots_global

        # store on the module's device
        self.knots_l.copy_(new_knots_l)
        self.knots_r.copy_(new_knots_r)
        self.fitted = True


    def _interp_piecewise_linear(self, l, Lk, Rk):
        """
        Vectorized piecewise-linear interpolation + slope:
          inputs:
            l  : [B] norms to map
            Lk : [K] source knots (sorted, strictly increasing)
            Rk : [K] target knots
          returns:
            r     : [B] mapped radii
            slope : [B] dr/dl per-sample
        """
        K = Lk.numel()
        # find interval indices i such that Lk[i] <= l < Lk[i+1]
        idx = torch.searchsorted(Lk, l.clamp(min=Lk[0], max=Lk[-1]-1e-12), right=False).clamp(1, K-1)
        i0 = idx - 1
        i1 = idx
        L0 = Lk[i0]; L1 = Lk[i1]
        R0 = Rk[i0]; R1 = Rk[i1]
        # slope and interpolation
        denom = (L1 - L0).clamp_min(1e-12)
        s = (R1 - R0) / denom
        r = R0 + s * (l - L0)
        return r, s

    def forward(self, z, y=None):
        """
        z: [B, D]; y: [B] or None
        If not fitted -> identity (z, 0.0 logdet)
        If fitted     -> apply class-conditional (if n_classes set & y provided) or global
        """
        B, D = z.shape
        assert D == self.dim
        if not self.fitted:
            return z, z.new_zeros(B)

        r_in = z.norm(dim=1)  # [B]
        eps = self.eps

        if self.n_classes is not None and y is not None:
            z_new = torch.empty_like(z)
            logdet = torch.empty(B, device=z.device)
            for c in torch.unique(y):
                mask = (y == c)
                r_map, slope = self._interp_piecewise_linear(
                    r_in[mask],
                    self.knots_l[c],
                    self.knots_r[c]
                )
                scale = (r_map / (r_in[mask] + eps)).unsqueeze(1)
                z_new[mask] = scale * z[mask]
                logdet[mask] = ( (D-1) * torch.log(r_map / (r_in[mask] + eps)) + torch.log(slope + eps) )
        else:
            r_map, slope = self._interp_piecewise_linear(
                r_in,
                self.knots_l[0],
                self.knots_r[0]
            )
            scale = (r_map / (r_in + eps)).unsqueeze(1)
            z_new = scale * z
            logdet = ( (D-1) * torch.log(r_map / (r_in + eps)) + torch.log(slope + eps) )

        return z_new, logdet

# ---------- PVAE level (EDITED: added use_recalib flag) ----------
class PVAELevel(nn.Module):
    def __init__(self, input_num_dim, cat_cardinalities, latent_dim=64, context_dim=128,
                 n_planar=2, n_coupling=1, n_maf=1, n_classes=None,
                 poisson_scales=None, use_recalib=True):
        super().__init__()
        self.latent_dim = latent_dim
        self.n_classes = n_classes
        self.use_recalib = bool(use_recalib)
        self.register_buffer("poisson_scales", poisson_scales if poisson_scales is not None else torch.ones(input_num_dim))

        # embeddings
        self.embeddings = nn.ModuleList([nn.Embedding(c, min(32, (c+1)//2)) for c in cat_cardinalities])
        emb_dim = sum(e.embedding_dim for e in self.embeddings) if self.embeddings else 0

        # encoder
        self.enc = nn.Sequential(nn.Linear(input_num_dim + emb_dim, 256), nn.ReLU(),
                                 nn.Linear(256, context_dim), nn.ReLU())
        self.mu_head = nn.Linear(context_dim, latent_dim)
        self.logvar_head = nn.Linear(context_dim, latent_dim)

        # flows
        self.planars = nn.ModuleList([PlanarFlow(latent_dim) for _ in range(n_planar)])
        self.couplings = nn.ModuleList([AffineCoupling(latent_dim, 256, context_dim, mask_even=(i%2==0)) for i in range(n_coupling)])
        self.mafs = nn.ModuleList([MAF(latent_dim) for _ in range(n_maf)])

        # recalib (can be bypassed via self.use_recalib=False)
        self.recalib = LRQuantileRadialCalibrator(latent_dim, n_classes=n_classes)

        # decoder
        self.dec = nn.Sequential(nn.Linear(latent_dim,128), nn.ReLU(),
                                 nn.Linear(128, input_num_dim))

    def embed_cat(self, xcat):
        if len(self.embeddings)==0 or xcat.shape[1]==0: return None
        return torch.cat([emb(xcat[:,j]) for j,emb in enumerate(self.embeddings)], dim=1)

    def encode(self, xnum, xcat):
        emb = self.embed_cat(xcat)
        xin = torch.cat([xnum, emb], dim=1) if emb is not None else xnum
        h = self.enc(xin)
        mu = self.mu_head(h)
        logvar = self.logvar_head(h).clamp(-10, 10)
        return mu, logvar, h

    @staticmethod
    def rsample(mu, logvar):
        return mu + torch.exp(0.5*logvar) * torch.randn_like(mu)

    def flows_forward(self, z, context):
        sum_ld = torch.zeros(z.size(0), device=z.device)
        for i,pf in enumerate(self.planars):
            z, ld = pf(z); sum_ld += ld
            if i < len(self.couplings): z, ld2 = self.couplings[i](z, context); sum_ld += ld2
            if i < len(self.mafs):      z, ld3 = self.mafs[i](z);              sum_ld += ld3
        return z, sum_ld

    def decode_rate(self, z): return F.softplus(self.dec(z)) + 1e-6

    @staticmethod
    def std_normal_logprob(z): return -0.5 * (z.pow(2) + np.log(2*pi)).sum(dim=1)
    @staticmethod
    def gaussian_logprob(z, mu, logvar): return -0.5 * ((z-mu).pow(2)/logvar.exp() + logvar + np.log(2*pi)).sum(dim=1)

    def poisson_nll(self, xnum, rate):
        target = torch.clamp(xnum * self.poisson_scales, min=0.0)
        return F.poisson_nll_loss(rate.log(), target, log_input=True, full=True, reduction='none').sum(dim=1)

    def forward(self, xnum, xcat, y=None, deterministic=False):
        mu, logvar, ctx = self.encode(xnum, xcat)
        z0 = mu if deterministic else self.rsample(mu, logvar)
        zK, ld_flow = self.flows_forward(z0, ctx)

        if self.use_recalib:
            zR, ld_recal = self.recalib(zK, y=y)
        else:
            zR, ld_recal = zK, zK.new_zeros(zK.size(0))

        rate = self.decode_rate(zR)
        return {"rate":rate, "mu":mu, "logvar":logvar, "z0":z0, "zK":zK, "zR":zR,
                "sum_logdet":ld_flow+ld_recal, "context":ctx}

# ---------- Bridge flow (conditional RealNVP, MLE) ----------
class BridgeFlow(nn.Module):
    def __init__(self, dim, context_dim=128, n_coupling=2):
        super().__init__()
        self.couplings = nn.ModuleList([AffineCoupling(dim, 256, context_dim, mask_even=(i%2==0)) for i in range(n_coupling)])
    def log_prob(self, z_in, context):
        z = z_in; sum_ld = torch.zeros(z.size(0), device=z.device)
        for c in reversed(self.couplings):
            D = z.shape[1]; idx = torch.arange(D, device=z.device)
            mask = (idx%2==0) if c.mask_even else (idx%2==1)
            z1 = z[:,mask]; z2 = z[:,~mask]
            h = torch.cat([z1, context], dim=1); out = c.net(h); t,s = out[:,:z2.shape[1]], out[:,z2.shape[1]:]; s = torch.tanh(s)
            z2 = (z2 - t) * torch.exp(-s)
            z_base = torch.empty_like(z); z_base[:,mask]=z1; z_base[:,~mask]=z2
            z = z_base; sum_ld += (-s).sum(dim=1)
        log_pz = -0.5 * (z**2 + np.log(2*pi)).sum(dim=1)
        return log_pz + sum_ld

# ---------- Chain container with proxy heads ----------
class PVAEChain(nn.Module):
    def __init__(self, input_dims, cat_cardinalities, latent_dim=64, n_classes=None, poisson_scales_list=None):
        super().__init__()
        assert len(input_dims) == 3 and len(poisson_scales_list) == 3
        self.l1 = PVAELevel(input_dims[0], cat_cardinalities, latent_dim, n_classes=None, poisson_scales=poisson_scales_list[0])
        self.l2 = PVAELevel(input_dims[1], cat_cardinalities, latent_dim, n_classes=None, poisson_scales=poisson_scales_list[1])
        self.l3 = PVAELevel(input_dims[2], cat_cardinalities, latent_dim, n_classes=n_classes, poisson_scales=poisson_scales_list[2])
        self.b12 = BridgeFlow(latent_dim, context_dim=128, n_coupling=2)
        self.b23 = BridgeFlow(latent_dim, context_dim=128, n_coupling=2)
        self.proxy_multi = nn.Linear(latent_dim, n_classes)
        self.proxy_bin   = nn.Linear(latent_dim, 2)

    def forward(self, xnum1, xnum2, xnum3, xcat, y=None, deterministic=False):
        o1 = self.l1(xnum1, xcat, y=None, deterministic=deterministic)
        o2 = self.l2(xnum2, xcat, y=None, deterministic=deterministic)
        o3 = self.l3(xnum3, xcat, y=y,    deterministic=deterministic)
        lp12 = self.b12.log_prob(o1["zR"], o2["context"])
        lp23 = self.b23.log_prob(o2["zR"], o3["context"])
        return o1, o2, o3, lp12, lp23

# === Convenience: inference features for PPO & meta ===
@torch.no_grad()
def chain_infer_features(chain, xb_num1, xb_num2, xb_num3, xb_cat, y=None, deterministic=True):
    """
    Returns per-sample dict with:
      - probs: [B,C] (softmax of proxy_multi)
      - entropy: [B] predictive entropy
      - flow_logp: [B] (lp12+lp23)
      - sparsity_med: [B] median decoded rate (L3)
      - sparsity_frac3x: [B] fraction of decoded rates > 3x median (L3)
    """
    chain.eval()
    o1, o2, o3, lp12, lp23 = chain(xb_num1, xb_num2, xb_num3, xb_cat, y=y, deterministic=deterministic)
    logits = chain.proxy_multi(o3["zR"])
    probs  = F.softmax(logits, dim=1)
    entropy = -(probs.clamp_min(1e-8)*probs.clamp_min(1e-8).log()).sum(dim=1)

    flow_logp = (lp12 + lp23)

    rate = o3["rate"]
    med  = rate.median(dim=1, keepdim=True).values
    frac_hi = (rate > 3.0*med).float().mean(dim=1)

    return {
        "probs": probs, "entropy": entropy,
        "flow_logp": flow_logp,
        "sparsity_med": med.squeeze(1),
        "sparsity_frac3x": frac_hi
    }

# ---------- Regularizers ----------
def pfgm_regularizer(z0, zK, sum_logdet, rate, tau_logdet=6.0, tau_rad=1.5):
    """
    Stable NF regularizer inspired by PFGM ideas:
      - penalize excessive local volume change (|logdet| beyond tau_logdet),
      - discourage runaway radial expansion (||zK|| much larger than sqrt(D)),
      - keep Poisson rates from exploding relative to batch median.
    """
    D = zK.size(1)
    # volume change control
    vol_pen = F.relu(sum_logdet.abs() - tau_logdet).pow(2).mean()
    # radial expansion control (reference ~ sqrt(D))
    rad = torch.norm(zK, dim=1)
    rad_pen = F.relu(rad - tau_rad * math.sqrt(D)).pow(2).mean()
    # sparse rate control (relative to median per sample)
    median = rate.median(dim=1, keepdim=True).values
    spike_pen = F.relu(rate - 3.0 * median).mean()
    return spike_pen + 0.1 * vol_pen + 0.05 * rad_pen


def latent_calib_nll(z, dim):
    r = torch.norm(z, dim=1, keepdim=True)
    return ( -((dim-1)*torch.log(r+1e-8) - 0.5*(r**2)) ).mean()

# === Simple UCB bandit over classes (RL at the beginning) ===
class RLUCBSampler:
    """
    Upper-Confidence Bound bandit on classes.
    We maintain a 'need score' per class (higher when proxy-val loss is high),
    then compute sampling probs p ~ softmax(q + bonus).
    """
    def __init__(self, n_classes, c_bonus=1.0, temperature=1.0, eps_floor=1e-3):
        self.n = np.zeros(n_classes, dtype=np.float64)   # pulls
        self.q = np.zeros(n_classes, dtype=np.float64)   # value (need)
        self.w_class = np.ones(n_classes, dtype=np.float64)
        self.t = 0
        self.c_bonus = c_bonus
        self.temperature = temperature
        self.eps_floor = eps_floor

    def get_probs(self):
        self.t += 1
        bonus = np.sqrt(np.log(self.t + 1.0) / (self.n + 1.0))
        score = self.q + self.c_bonus * bonus
        # softmax with temperature, floor to avoid zeros
        score = score - score.max()
        p = np.exp(score / max(self.temperature, 1e-6))
        p = p / p.sum()
        p = np.maximum(p, self.eps_floor)
        p = p / p.sum()
        return p

    def update(self, rewards, pulls=None, alpha=0.5):
        """
        rewards: array [C] in [0,1], higher means 'needs more sampling'
        pulls: expected pulls per class this epoch; if None, increments by 1
        alpha: EMA factor for q
        """
        rewards = np.asarray(rewards, dtype=np.float64)
        if pulls is None:
            pulls = np.ones_like(rewards)
        self.n += pulls
        # EMA update (stabilizes noise)
        self.q = alpha * self.q + (1.0 - alpha) * rewards

# ---------- Train ----------
latent_dim = 64
chain = PVAEChain(input_dims=[input_num_dim_l1, input_num_dim_l2, input_num_dim_l3],cat_cardinalities=cat_cardinalities,latent_dim=latent_dim,n_classes=n_classes,poisson_scales_list=[ps1.float(), ps2.float(), ps3.float()]
).to(device)


# param groups: lower LR for bridges
bridge_params = list(chain.b12.parameters()) + list(chain.b23.parameters())
main_params = [p for n,p in chain.named_parameters() if not n.startswith('b12') and not n.startswith('b23')]

opt = torch.optim.Adam([
    {'params': main_params,   'lr': 1e-3},
    {'params': bridge_params, 'lr': 5e-4}
])
# --- AMP (PyTorch 2.x friendly) ---
from contextlib import nullcontext
if device.type == 'cuda':
    amp_autocast = lambda: torch.amp.autocast('cuda')
    scaler = torch.amp.GradScaler('cuda', enabled=True)
else:
    amp_autocast = nullcontext
    scaler = None

# --- RL bandit init (uniform start) ---
bandit = RLUCBSampler(n_classes=n_classes, c_bonus=0.3, temperature=1.1, eps_floor=0.03)
# store per-class multipliers (used next epoch in proxy losses)
bandit.w_class = np.ones(n_classes, dtype=np.float64)

# base class frequencies (reuse y_train_np and counts you already loaded below)
# if 'counts' not yet defined above this point, move these three lines to
# *after* you compute 'counts' for class_weights_mc.

EPOCHS = 30
patience = 8; best_score = float('inf'); no_improve = 0

def ramp(start, end, cur, total):
    t = min(1.0, cur/total); return start + (end-start)*t

logs = {"tr":[], "va":[]}
chain.train()

# === Class weights for proxy heads (multiclass + binary) ===
# Use inverse-frequency with mean ~ 1 (stable scale).
import numpy as np, torch

y_train_np = np.load('y_train_sir.npy').astype(int)
num_classes_proxy = int(y_train_np.max() + 1)
counts = np.bincount(y_train_np, minlength=num_classes_proxy)
w_mc = (counts.sum() / (counts + 1e-8))  # inverse freq
w_mc = w_mc * (num_classes_proxy / w_mc.sum())  # normalize mean ~ 1
class_weights_mc = torch.tensor(w_mc, dtype=torch.float32, device=device)

# base distribution stats (used by RL weight scheduler)
base_counts = counts.astype(np.float64)
base_freq   = base_counts / base_counts.sum()
attack_mass = float(base_freq[1:].sum() + 1e-12)  # avoid divide-by-zero

# Binary (benign vs attack) weights for the proxy_bin head
y_bin_np = (y_train_np != 0).astype(int)
n_neg = int((y_bin_np == 0).sum())  # benign
n_pos = int((y_bin_np == 1).sum())  # attack
w_benign = (n_neg + n_pos) / (2.0 * n_neg + 1e-8)
w_attack = (n_neg + n_pos) / (2.0 * n_pos + 1e-8)
bin_class_weights = torch.tensor([w_benign, w_attack], dtype=torch.float32, device=device)

for epoch in range(1, EPOCHS+1):
    # schedules
    alpha_proxy = ramp(0.1, 0.6, epoch-1, 15)  # ramp up over 10 epochs
    beta_nll    = 0.0 if epoch <= 5 else ramp(0.0, 1.0, epoch-5, 5)  # warm-up 0→1 over epochs 6–10
    # === RL: read current class multipliers to scale proxy losses (no resampling) ===
    w_rl_class = torch.tensor(bandit.w_class, dtype=torch.float32, device=device)

    # combine base weights with a *small* RL nudging
    bandit_mult_t = torch.clamp(w_rl_class, 0.95, 1.05)      # very narrow range
    lambda_rl = 0.30                                         # blend factor (0 = ignore RL, 1 = fully trust)
    class_weights_epoch = (1.0 - lambda_rl) * class_weights_mc + lambda_rl * (class_weights_mc * bandit_mult_t)


    # effective weights for the multiclass proxy
    eff_class_weights_mc = class_weights_mc * w_rl_class

    # map to binary head: keep benign fixed at 1.0, scale attack by avg attack multiplier
    avg_attack_mult = (w_rl_class[1:] * torch.tensor(base_freq[1:], device=device)).sum() / torch.tensor(attack_mass, device=device)
    eff_bin_weights = torch.tensor([bin_class_weights[0], bin_class_weights[1] * avg_attack_mult], dtype=torch.float32, device=device)


    tr_elbo=tr_pf=tr_nll=tr_pxm=tr_pxb=0.0; nobs=0
    for xb_num1, xb_num2, xb_num3, xb_cat, yb in DL_train:
        xb_num1 = xb_num1.to(device, non_blocking=True)
        xb_num2 = xb_num2.to(device, non_blocking=True)
        xb_num3 = xb_num3.to(device, non_blocking=True)
        xb_cat  = xb_cat.to(device,  non_blocking=True)
        yb      = yb.to(device,      non_blocking=True).long()

        with amp_autocast():
            o1, o2, o3, lp12, lp23 = chain(xb_num1, xb_num2, xb_num3, xb_cat, y=yb, deterministic=False)

            def elbo(level_mod, o, xnum):
                recon = level_mod.poisson_nll(xnum, o["rate"]).mean()
                kl = (level_mod.gaussian_logprob(o["z0"], o["mu"], o["logvar"])
                      - o["sum_logdet"]
                      - level_mod.std_normal_logprob(o["zR"])).mean()
                return recon + kl

            elb = elbo(chain.l1, o1, xb_num1) + elbo(chain.l2, o2, xb_num2) + elbo(chain.l3, o3, xb_num3)

            # Bridge NLL (maximize => minimize negative)
            nll_flow = -(lp12.mean() + lp23.mean())

            # PFGM-ish regularizers per level
            pf = (
                pfgm_regularizer(o1["z0"], o1["zR"], o1["sum_logdet"], o1["rate"])
              + pfgm_regularizer(o2["z0"], o2["zR"], o2["sum_logdet"], o2["rate"])
              + pfgm_regularizer(o3["z0"], o3["zR"], o3["sum_logdet"], o3["rate"])
            )

            # Top-level latent calibration encouragement (kept)
            calib_term = 0.5 * latent_calib_nll(o3["zR"], latent_dim)
            # --- proxy heads with shape guards ---
            logits_m = chain.proxy_multi(o3["zR"])
            B_logits = logits_m.size(0)

            # ensure targets are 1-D and same length as logits
            yb_use = yb.view(-1)[:B_logits]
            pxm = F.cross_entropy(logits_m, yb_use, weight=class_weights_epoch)

            logits_b = chain.proxy_bin(o3["zR"])
            yb_bin_use = (yb_use != 0).long()          # recompute from sliced y
            pxb = F.cross_entropy(logits_b, yb_bin_use, weight=eff_bin_weights)

            loss = elb + beta_nll * nll_flow + pf + calib_term + alpha_proxy * (pxm + 0.5 * pxb)

        opt.zero_grad(set_to_none=True)
        if scaler is not None:
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(chain.parameters(), 5.0)
            scaler.step(opt); scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(chain.parameters(), 5.0)
            opt.step()

        #B = xb_num1.size(0)  # <-- use any per-level tensor to get batch size
        B = B_logits
        nobs += B
        tr_elbo += elb.item() * B
        tr_pf   += pf.item() * B
        tr_nll  += nll_flow.item() * B
        tr_pxm  += pxm.item() * B
        tr_pxb  += pxb.item() * B

    tr_elbo/=nobs; tr_pf/=nobs; tr_nll/=nobs; tr_pxm/=nobs; tr_pxb/=nobs

    # ===== validation =====
    chain.eval()
    vnobs = 0; v_elbo = v_pf = v_nll = v_pxm = v_pxb = 0.0
    val_ce_sum = torch.zeros(n_classes, device=device)
    val_ce_cnt = torch.zeros(n_classes, device=device)

    with torch.no_grad():
        for vx_num1, vx_num2, vx_num3, vx_cat, vy in DL_val:
            vx_num1 = vx_num1.to(device, non_blocking=True)
            vx_num2 = vx_num2.to(device, non_blocking=True)
            vx_num3 = vx_num3.to(device, non_blocking=True)
            vx_cat  = vx_cat.to(device,  non_blocking=True)
            vy      = vy.to(device,      non_blocking=True).long()

            with amp_autocast():
                p1, p2, p3, lp12v, lp23v = chain(vx_num1, vx_num2, vx_num3, vx_cat, y=vy, deterministic=True)

                def elbo(level_mod, o, xnum):
                    recon = level_mod.poisson_nll(xnum, o["rate"]).mean()
                    kl = (level_mod.gaussian_logprob(o["z0"], o["mu"], o["logvar"])
                          - o["sum_logdet"]
                          - level_mod.std_normal_logprob(o["zR"])).mean()
                    return recon + kl

                # generative pieces
                elb  = elbo(chain.l1, p1, vx_num1) + elbo(chain.l2, p2, vx_num2) + elbo(chain.l3, p3, vx_num3)
                nllv = -(lp12v.mean() + lp23v.mean())
                pfv  = (
                    pfgm_regularizer(p1["z0"], p1["zR"], p1["sum_logdet"], p1["rate"])
                  + pfgm_regularizer(p2["z0"], p2["zR"], p2["sum_logdet"], p2["rate"])
                  + pfgm_regularizer(p3["z0"], p3["zR"], p3["sum_logdet"], p3["rate"])
                )

                # proxy heads (shape-guarded) — use same weights as train if you want
                logits_vm = chain.proxy_multi(p3["zR"])
                Bv = logits_vm.size(0)
                vy_use = vy.view(-1)[:Bv]

                pxmv = F.cross_entropy(logits_vm, vy_use, weight=class_weights_mc)

                logits_vb = chain.proxy_bin(p3["zR"])
                vy_bin_use = (vy_use != 0).long()
                pxbv = F.cross_entropy(logits_vb, vy_bin_use, weight=bin_class_weights)

                # === RL reward accumulation (unweighted per-sample CE by class) ===
                ce_per = F.cross_entropy(logits_vm, vy_use, reduction='none')  # unweighted
                for c in range(n_classes):
                    m = (vy_use == c)
                    if m.any():
                        val_ce_sum[c] += ce_per[m].sum()
                        val_ce_cnt[c] += m.sum()


            # accumulators
            B = Bv
            vnobs += B
            v_elbo += elb.item()  * B
            v_nll  += nllv.item() * B
            v_pf   += pfv.item()  * B
            v_pxm  += pxmv.item() * B
            v_pxb  += pxbv.item() * B

    # averages
    v_elbo /= vnobs; v_nll /= vnobs; v_pf /= vnobs; v_pxm /= vnobs; v_pxb /= vnobs
    chain.train()
    # === RL: compute rewards and update next-epoch multipliers (no resampling) ===
    with torch.no_grad():
        avg_ce = (val_ce_sum / (val_ce_cnt + 1e-9)).detach().cpu().numpy()

        # normalize to [0,1]; higher CE => more need
        ce_min, ce_max = float(np.min(avg_ce)), float(np.max(avg_ce))
        rewards = (avg_ce - ce_min) / (ce_max - ce_min) if ce_max > ce_min else np.zeros_like(avg_ce)

        # weight-only RL → no resampling → let bandit handle its own pulls
        bandit.update(rewards=rewards, pulls=None, alpha=0.5)

        # --- COOL, SLOW MULTIPLIER UPDATE (replaces your old 0.7–1.4 mapping) ---
        if epoch >= 6 and (epoch % 2 == 0):  # start after warmup, update every 2 epochs
            # tiny band to avoid blowing up proxies
            w_target = 0.95 + 0.10 * rewards       # map [0,1] → [0.95, 1.05]
            bandit.w_class = 0.9 * bandit.w_class + 0.1 * w_target  # slow EMA
            bandit.w_class = np.clip(bandit.w_class, 0.95, 1.05)


        # convert rewards -> gentle, bounded multipliers for next epoch
        kappa = 0.6
        r = rewards
        r_mean = r.mean()
        w_new = 1.0 + kappa * (r - r_mean)
        w_new = np.clip(w_new, 0.6, 1.8)
        w_new[0] = 1.0           # never distort Benign
        bandit.w_class = w_new


    logs["tr"].append((tr_elbo,tr_nll,tr_pf,tr_pxm,tr_pxb))
    logs["va"].append((v_elbo,v_nll,v_pf,v_pxm,v_pxb))
    print(f"[Ep {epoch:02d}] ELBO tr/val {tr_elbo:.1f}/{v_elbo:.1f} | NLL tr/val {tr_nll:.1f}/{v_nll:.1f} | PF tr/val {tr_pf:.1f}/{v_pf:.1f} | ProxyM {tr_pxm:.3f}/{v_pxm:.3f} α={alpha_proxy:.2f} | β={beta_nll:.2f}")

    score = v_elbo + v_nll  # early-stop on generative fit
    if score < best_score - 1e-3:
        best_score = score; no_improve = 0
        torch.save(chain.state_dict(), "artifacts/best_chain.pth")
    else:
        no_improve += 1
        if no_improve >= patience:
            print("⛔ Early stop"); break

# reload best
chain.load_state_dict(torch.load("artifacts/best_chain.pth", map_location=device))
chain.eval()
print("✅ Multi-level Flow-P(VAE) chain trained & loaded.")

# We fit on top-level PRE-recalibration latents (zK) and build a class-conditional radial map.

def collect_top_latents(chain, DL, device, max_points=1_000_000):
    chain.eval()
    Zk_list, Y_list = [], []
    seen = 0
    with torch.no_grad():
        for xb_num1, xb_num2, xb_num3, xb_cat, yb in DL:
            xb_num1 = xb_num1.to(device).float()
            xb_num2 = xb_num2.to(device).float()
            xb_num3 = xb_num3.to(device).float()
            xb_cat  = xb_cat.to(device).long()
            yb      = yb.to(device).long()
            _o1, _o2, o3, _lp12, _lp23 = chain(xb_num1, xb_num2, xb_num3, xb_cat, y=yb, deterministic=True)
            Zk_list.append(o3["zK"].detach().cpu()); Y_list.append(yb.detach().cpu())
            seen += xb_num1.size(0)
            if seen >= max_points: break
    return torch.cat(Zk_list, dim=0), torch.cat(Y_list, dim=0)

print("Fitting LR recalibrator on validation split…")
Zk_val, y_val_t = collect_top_latents(chain, DL_val, device)
# Fit class-conditional LR on val latents
chain.l3.recalib.fit(Zk_val, y_val_t, base_samples=200_000)
# save calibrator state for reuse
torch.save(
    {
        "knots_l": chain.l3.recalib.knots_l.cpu(),
        "knots_r": chain.l3.recalib.knots_r.cpu(),
        "n_classes": chain.l3.recalib.n_classes,
        "n_knots": chain.l3.recalib.n_knots,
        "dim": chain.l3.recalib.dim,
        "fitted": chain.l3.recalib.fitted
    },
    "artifacts/lr_recalibrator.pth"
)
print("✅ LR recalibrator fitted & saved.")

# === BLOCK 3A — Register PVAE base learners (generative experts) ===
BASES = []

# Base A: recalibrated (the model you just trained)
baseA = PVAEChain(
    input_dims=[input_num_dim_l1, input_num_dim_l2, input_num_dim_l3],
    cat_cardinalities=cat_cardinalities,
    latent_dim=latent_dim,
    n_classes=n_classes,
    poisson_scales_list=[ps1.float(), ps2.float(), ps3.float()]
).to(device)
baseA.load_state_dict(torch.load("artifacts/best_chain.pth", map_location=device))
# keep recalibrator ON (already fitted)
BASES.append(("A_recal", baseA))

# Base B: same weights, but recalibrator OFF at top level → different decision surface
baseB = PVAEChain(
    input_dims=[input_num_dim_l1, input_num_dim_l2, input_num_dim_l3],
    cat_cardinalities=cat_cardinalities,
    latent_dim=latent_dim,
    n_classes=n_classes,
    poisson_scales_list=[ps1.float(), ps2.float(), ps3.float()]
).to(device)
baseB.load_state_dict(torch.load("artifacts/best_chain.pth", map_location=device))
# disable recalib at L3
baseB.l3.use_recalib = False
BASES.append(("B_norecal", baseB))

# Base C: same checkpoint, but all latent flows disabled (planar + MAF off) → different inductive bias
baseC = PVAEChain(
    input_dims=[input_num_dim_l1, input_num_dim_l2, input_num_dim_l3],
    cat_cardinalities=cat_cardinalities,
    latent_dim=latent_dim,
    n_classes=n_classes,
    poisson_scales_list=[ps1.float(), ps2.float(), ps3.float()]
).to(device)
baseC.load_state_dict(torch.load("artifacts/best_chain.pth", map_location=device))
# disable flows at every level
import torch.nn as nn
baseC.l1.planars = nn.ModuleList([]); baseC.l1.mafs = nn.ModuleList([])
baseC.l2.planars = nn.ModuleList([]); baseC.l2.mafs = nn.ModuleList([])
baseC.l3.planars = nn.ModuleList([]); baseC.l3.mafs = nn.ModuleList([])
BASES.append(("C_noflow", baseC))

print(f"✅ Registered generative bases: {[name for name,_ in BASES]} (K={len(BASES)})")

# BLOCK 4 — Encode latents, save for XGB
import numpy as np, torch, contextlib
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# load splits exactly as trained
Xnum_train = np.load('Xnum_train_sir.npy')
Xnum_val   = np.load('Xnum_val_sir.npy')
Xnum_test  = np.load('Xnum_test_sir.npy')
Xcat_train = np.load('Xcat_train_sir.npy')
Xcat_val   = np.load('Xcat_val_sir.npy')
Xcat_test  = np.load('Xcat_test_sir.npy')
y_train    = np.load('y_train_sir.npy')
y_val      = np.load('y_val_sir.npy')
y_test     = np.load('y_test_sir.npy')

# choose the right context manager for AMP
autocast_ctx = torch.cuda.amp.autocast if device.type == "cuda" else contextlib.nullcontext

def encode_latents(xn1, xn2, xn3, xc, y, batch: int = 4096) -> np.ndarray:
    """Encode top-level recalibrated latents deterministically (ε=0), class-conditional LR applied."""
    chain.eval()
    Z = []
    with torch.no_grad():
        for i in range(0, len(xn1), batch):
            xb_num1 = torch.from_numpy(xn1[i:i+batch]).to(device, non_blocking=True).float()
            xb_num2 = torch.from_numpy(xn2[i:i+batch]).to(device, non_blocking=True).float()
            xb_num3 = torch.from_numpy(xn3[i:i+batch]).to(device, non_blocking=True).float()
            xb_cat  = torch.from_numpy(xc[i:i+batch]).to(device, non_blocking=True).long()
            yb      = torch.from_numpy(y[i:i+batch]).to(device, non_blocking=True).long()
            _, _, out3, _, _ = chain(xb_num1, xb_num2, xb_num3, xb_cat, y=yb, deterministic=True)
            Z.append(out3["zR"].detach().cpu().numpy())
    return np.concatenate(Z, axis=0)

# load per-level numerics (exactly as saved in Block 1)
Xnum1_train = np.load('Xnum_l1_train_sir.npy')
Xnum2_train = np.load('Xnum_l2_train_sir.npy')
Xnum3_train = np.load('Xnum_l3_train_sir.npy')

Xnum1_val   = np.load('Xnum_l1_val_sir.npy')
Xnum2_val   = np.load('Xnum_l2_val_sir.npy')
Xnum3_val   = np.load('Xnum_l3_val_sir.npy')

Xnum1_test  = np.load('Xnum_l1_test_sir.npy')
Xnum2_test  = np.load('Xnum_l2_test_sir.npy')
Xnum3_test  = np.load('Xnum_l3_test_sir.npy')

Z_train = encode_latents(Xnum1_train, Xnum2_train, Xnum3_train, Xcat_train, y_train, batch=8192)
Z_val   = encode_latents(Xnum1_val,   Xnum2_val,   Xnum3_val,   Xcat_val,   y_val,   batch=8192)
Z_test  = encode_latents(Xnum1_test,  Xnum2_test,  Xnum3_test,  Xcat_test,  y_test,  batch=8192)

np.save("Z_train.npy", Z_train); np.save("Z_val.npy", Z_val); np.save("Z_test.npy", Z_test)
print("Saved fresh Z_* latents:", Z_train.shape, Z_val.shape, Z_test.shape)

# === [NEW] Tiny post-hoc recalibration MLP (residual warp) ===
import torch.nn as nn, torch.optim as optim

class TinyWarp(nn.Module):
    def __init__(self, dim, hidden=128, scale=0.05):  # scale 0.05 (was 0.10)
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden//2), nn.ReLU(),
            nn.Linear(hidden//2, dim)
        )
        self.scale = scale
        # small init so it's near-identity at start
        for m in self.net.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, a=0.2)
                nn.init.zeros_(m.bias)

    def forward(self, z):
        return z + self.scale * self.net(z)

# Train on train→val with a linear probe objective
dimZ = Z_train.shape[1]
warp = TinyWarp(dimZ, hidden=128, scale=0.1).to(device)
clf  = nn.Linear(dimZ, int(np.max(y_train)+1)).to(device)

opt = optim.Adam(list(warp.parameters()) + list(clf.parameters()), lr=1e-3, weight_decay=1e-5)
loss_fn = nn.CrossEntropyLoss()

Ztr = torch.from_numpy(Z_train).float().to(device)
Zva = torch.from_numpy(Z_val).float().to(device)
ytr = torch.from_numpy(y_train).long().to(device)
yva = torch.from_numpy(y_val).long().to(device)

# Class weights for the warp's linear probe (same recipe as Block 3)
num_classes_warp = int(np.max(y_train) + 1)
counts_warp = np.bincount(y_train, minlength=num_classes_warp)
w_mc_warp = (counts_warp.sum() / (counts_warp + 1e-8))
w_mc_warp = w_mc_warp * (num_classes_warp / w_mc_warp.sum())
warp_class_weights = torch.tensor(w_mc_warp, dtype=torch.float32, device=device)

loss_fn = nn.CrossEntropyLoss(weight=warp_class_weights)

best, patience, bad = float('inf'), 8, 0
for ep in range(1, 101):
    warp.train(); clf.train()
    idx = torch.randperm(Ztr.size(0), device=device)
    for i in range(0, Ztr.size(0), 4096):
        b = idx[i:i+4096]
        z = warp(Ztr[b]); logits = clf(z)
        loss = loss_fn(logits, ytr[b]) + 1e-3 * (z - Ztr[b]).pow(2).mean()  # small L2 to keep warp tame
        opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(list(warp.parameters()) + list(clf.parameters()), 2.0); opt.step()

    # val
    warp.eval(); clf.eval()
    with torch.no_grad():
        zv = warp(Zva); lv = loss_fn(clf(zv), yva).item()
    if lv < best - 1e-4:
        best, bad = lv, 0
        torch.save({'warp': warp.state_dict(), 'clf': clf.state_dict()}, "artifacts/warp_mlp.pth")
    else:
        bad += 1
        if bad >= patience:
            break

# load best & apply warp to all splits for XGB features
ckpt = torch.load("artifacts/warp_mlp.pth", map_location=device)
warp.load_state_dict(ckpt['warp']); warp.eval()
with torch.no_grad():
    Zw_train = warp(torch.from_numpy(Z_train).float().to(device)).cpu().numpy()
    Zw_val   = warp(torch.from_numpy(Z_val).float().to(device)).cpu().numpy()
    Zw_test  = warp(torch.from_numpy(Z_test).float().to(device)).cpu().numpy()

np.save("Zw_train.npy", Zw_train); np.save("Zw_val.npy", Zw_val); np.save("Zw_test.npy", Zw_test)
print("Saved warped Z_* latents for XGB:", Zw_train.shape, Zw_val.shape, Zw_test.shape)

# Optional: quick t-SNE sanity (subset to keep it speedy)
try:
    idx = np.random.default_rng(42).choice(len(Z_test), size=min(8000, len(Z_test)), replace=False)
    Z_emb = TSNE(n_components=2, init='pca', learning_rate='auto', perplexity=30).fit_transform(Z_test[idx])
    plt.figure(figsize=(7,6))
    scatter = plt.scatter(Z_emb[:,0], Z_emb[:,1], c=y_test[idx], s=3, alpha=0.8, cmap='tab20')
    plt.legend(*scatter.legend_elements(num=int(np.max(y_train)+1)), title="Class", fontsize=7, ncol=2)
    plt.title("t-SNE of top-level recalibrated z")
    plt.grid(True); plt.tight_layout(); plt.show()
except Exception as e:
    print("[t-SNE skipped]", e)

# === BLOCK 5 — Generative base ensemble + PPO weighting + XGBoost meta ===
import os, numpy as np, torch
from sklearn.metrics import accuracy_score, classification_report, log_loss, roc_auc_score, average_precision_score
from sklearn.preprocessing import label_binarize
from xgboost import XGBClassifier

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
K = len(BASES)

# ---------- helper: collect per-base outputs & diagnostics ----------
def collect_from_bases(BASES, Xnum1, Xnum2, Xnum3, Xcat, y, batch=8192):
    n = len(Xnum1); C = n_classes
    P = np.zeros((n, C, K), dtype=np.float32)      # probs per base
    ENT = np.zeros((n, K), dtype=np.float32)       # predictive entropy per base
    DENS = np.zeros((n, K), dtype=np.float32)      # flow log-prob per base
    SP_MED = np.zeros((n, K), dtype=np.float32)    # Poisson median rate per base
    SP_FR3 = np.zeros((n, K), dtype=np.float32)    # fraction >3x median

    with torch.no_grad():
        for s in range(0, n, batch):
            sl = slice(s, min(s+batch, n))
            xb1 = torch.from_numpy(Xnum1[sl]).to(device).float()
            xb2 = torch.from_numpy(Xnum2[sl]).to(device).float()
            xb3 = torch.from_numpy(Xnum3[sl]).to(device).float()
            xbc = torch.from_numpy(Xcat[sl]).to(device).long()
            yb  = torch.from_numpy(y[sl]).to(device).long()

            for k,(name,base) in enumerate(BASES):
                feats = chain_infer_features(base, xb1, xb2, xb3, xbc, y=yb, deterministic=True)
                P[sl, :, k]   = feats["probs"].cpu().numpy()
                ENT[sl, k]    = feats["entropy"].cpu().numpy()
                DENS[sl, k]   = feats["flow_logp"].cpu().numpy()
                SP_MED[sl, k] = feats["sparsity_med"].cpu().numpy()
                SP_FR3[sl, k] = feats["sparsity_frac3x"].cpu().numpy()
    return P, ENT, DENS, SP_MED, SP_FR3

# load splits we already saved (Block 4 did)
Xnum1_train = np.load('Xnum_l1_train_sir.npy')
Xnum2_train = np.load('Xnum_l2_train_sir.npy')
Xnum3_train = np.load('Xnum_l3_train_sir.npy')
Xcat_train  = np.load('Xcat_train_sir.npy'); y_train = np.load('y_train_sir.npy').astype(int)

Xnum1_val   = np.load('Xnum_l1_val_sir.npy')
Xnum2_val   = np.load('Xnum_l2_val_sir.npy')
Xnum3_val   = np.load('Xnum_l3_val_sir.npy')
Xcat_val    = np.load('Xcat_val_sir.npy');   y_val   = np.load('y_val_sir.npy').astype(int)

Xnum1_test  = np.load('Xnum_l1_test_sir.npy')
Xnum2_test  = np.load('Xnum_l2_test_sir.npy')
Xnum3_test  = np.load('Xnum_l3_test_sir.npy')
Xcat_test   = np.load('Xcat_test_sir.npy');  y_test  = np.load('y_test_sir.npy').astype(int)

print("[Ensemble] collecting base outputs ...")
P_tr, ENT_tr, DENS_tr, SPM_tr, SPF_tr = collect_from_bases(BASES, Xnum1_train, Xnum2_train, Xnum3_train, Xcat_train, y_train)
P_va, ENT_va, DENS_va, SPM_va, SPF_va = collect_from_bases(BASES, Xnum1_val,   Xnum2_val,   Xnum3_val,   Xcat_val,   y_val)
P_te, ENT_te, DENS_te, SPM_te, SPF_te = collect_from_bases(BASES, Xnum1_test,  Xnum2_test,  Xnum3_test,  Xcat_test,  y_test)

# normalize densities per split (z-score) so thresholds are portable
def zscore(x): m = np.nanmean(x); s = np.nanstd(x) + 1e-8; return (x - m)/s
DENS_tr = zscore(DENS_tr); DENS_va = zscore(DENS_va); DENS_te = zscore(DENS_te)
# Use validation 20th percentile as OOD density threshold (slightly stricter than q10)
dens_gate = float(np.quantile(DENS_va, 0.20))
print(f"[PPO] density gate (q20 on val): {dens_gate:.3f}")

# ---------- Baseline meta (XGB-only, no RL) for reward deltas ----------
# Use the simple unweighted mean of base probs as baseline input
def mean_probs(P): return P.mean(axis=2)  # [N,C]
xgb_base = XGBClassifier(
    objective="multi:softprob", eval_metric="mlogloss", num_class=n_classes,
    tree_method="gpu_hist", predictor="gpu_predictor", gpu_id=0,
    random_state=42, n_jobs=0, max_depth=8, n_estimators=350, learning_rate=0.06
)
xgb_base.fit(mean_probs(P_tr), y_train, eval_set=[(mean_probs(P_va), y_val)], verbose=False)
BASE_PROB_VA = xgb_base.predict_proba(mean_probs(P_va))  # used inside PPO reward

# --- Class-aware FN costs from validation recall (pushes PPO to rescue weak classes) ---
from sklearn.metrics import confusion_matrix

y_va_pred_base = BASE_PROB_VA.argmax(axis=1)
cm = confusion_matrix(y_val, y_va_pred_base, labels=list(range(n_classes)))
with np.errstate(divide='ignore', invalid='ignore'):
    recall_va = np.diag(cm) / cm.sum(axis=1).clip(1, None)   # per-class recall on val
recall_va = np.nan_to_num(recall_va, nan=0.0)

# === gentler FN costs to avoid attack-overweight ===
# Map low recall -> modest FN cost, keep range tighter (1.0..2.5), and smooth
fn_cost_vec = 1.0 + 1.5 * (1.0 - recall_va)
# optional smoothing toward global mean so no class explodes
mu = fn_cost_vec.mean()
fn_cost_vec = 0.7 * fn_cost_vec + 0.3 * mu
# benign stays special (we handle FP separately)
fn_cost_vec[0] = 1.0
print("[PPO] class recall (val):", np.round(recall_va, 3))
print("[PPO] class-aware fn_cost_vec (gentle):", np.round(fn_cost_vec, 2))

# --- targeted boost for class 6 (weak recall) ---
fn_cost_vec[6] = float(fn_cost_vec[6] * 1.35)
print("[PPO] boosted fn_cost for class 6 ->", round(fn_cost_vec[6], 2))

# --- SPLIT VALIDATION FOR PPO TRAINING (avoid overfitting to the full val stream) ---
rng_local = np.random.default_rng(13)
idx = rng_local.permutation(len(y_val))
cut = int(0.8 * len(idx))
iv_tr, iv_ev = idx[:cut], idx[cut:]

# Slice validation tensors for PPO training
P_va_tr, ENT_va_tr, DENS_va_tr, SPM_va_tr, SPF_va_tr = P_va[iv_tr], ENT_va[iv_tr], DENS_va[iv_tr], SPM_va[iv_tr], SPF_va[iv_tr]
y_va_tr = y_val[iv_tr]
BASE_PROB_VA_tr = BASE_PROB_VA[iv_tr]

# ---------- PPO env (unique reward + constraints + stick-breaking) ----------
try:
    import gymnasium as gym
    from gymnasium import spaces
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
except Exception as e:
    raise RuntimeError("Please install PPO deps first: pip install 'stable-baselines3[extra]' gymnasium") from e

def softmax_last(x): x = x - np.max(x, axis=-1, keepdims=True); ex = np.exp(x); return ex/np.clip(ex.sum(axis=-1, keepdims=True),1e-8,None)

def stick_breaking(u):
    """
    u in R^{K-1} -> weights on K-simplex; numerically stable.
    """
    K1 = u.shape[-1]
    s = 1.0 / (1.0 + np.exp(-u))      # sigmoid
    rem = 1.0
    w = []
    for i in range(K1):
        wi = s[...,i] * rem
        w.append(wi)
        rem = rem * (1.0 - s[...,i])
    w.append(rem)
    return np.stack(w, axis=-1)

class StackStateFeaturizer:
    def __init__(self, C, K):
        self.C, self.K = C, K
    def make(self, P, ENT, DENS, SPM, SPF, i):
        """
        State vector for sample i:
          [ probs_flat(K*C) , var_across_bases(C) , ENT(K) , DENS(K) , SPM(K) , SPF(K) ]
        """
        probs_flat = P[i].reshape(-1)                   # K*C
        var_across = P[i].var(axis=1)                  # C
        return np.concatenate([probs_flat, var_across, ENT[i], DENS[i], SPM[i], SPF[i]]).astype(np.float32)

class WeightedStackEnv(gym.Env):
    metadata = {"render.modes": []}
    def __init__(self, P, ENT, DENS, SPM, SPF, y, base_prob_va,
                 fp_cost=5.0, fn_cost=1.0, ent_w=0.01, div_w=0.05, ood_w=0.5, calib_w=0.2, sparse_w=0.05,
                 dens_threshold=-1.0, seed=7, fn_cost_vec=None):
        super().__init__()
        assert P.ndim == 3
        self.P = P.astype(np.float32); self.ENT = ENT.astype(np.float32)
        self.DENS = DENS.astype(np.float32); self.SPM = SPM.astype(np.float32); self.SPF = SPF.astype(np.float32)
        self.y = y.astype(np.int64)
        self.N, self.C, self.K = P.shape
        self.target_w_entropy = 0.95 * float(np.log(self.K))
        self.feat = StackStateFeaturizer(self.C, self.K)
        self.fp_cost, self.fn_cost = float(fp_cost), float(fn_cost)
        # class-aware FN costs (defaults to all-ones if not provided)
        self.fn_cost_vec = np.ones(self.C, dtype=np.float32)
        if fn_cost_vec is not None:
            self.fn_cost_vec = np.asarray(fn_cost_vec, dtype=np.float32)
        self.ent_w, self.div_w, self.ood_w, self.calib_w, self.sparse_w = ent_w, div_w, ood_w, calib_w, sparse_w
        self.dens_threshold = float(dens_threshold)
        self.base_prob = base_prob_va.astype(np.float32)  # baseline XGB probs for reward delta (validation only)
        self.rng = np.random.default_rng(seed)
        # action = R^{K-1} (stick-breaking); observation sized accordingly
        obs_dim = self.K*self.C + self.C + self.K*4
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)
        self.action_space = spaces.Box(low=-6.0, high=6.0, shape=(self.K-1,), dtype=np.float32)
        self._i = 0

    def _obs(self): return self.feat.make(self.P, self.ENT, self.DENS, self.SPM, self.SPF, self._i)

    def reset(self, *, seed=None, options=None):
        if seed is not None: self.rng = np.random.default_rng(seed)
        self._i = self.rng.integers(0, self.N)
        return self._obs(), {}

    def step(self, action):
        w = stick_breaking(np.asarray(action).reshape(1,-1))[0]     # [K]
        probs_bases = self.P[self._i]                                # [C,K]
        probs_mix = (probs_bases @ w).clip(1e-8, 1-1e-8)             # [C]
        ytrue = int(self.y[self._i])

        # --- task gain: improvement over baseline XGB on this sample ---
        p_base = self.base_prob[self._i]                             # [C]
        ce_base = -np.log(np.clip(p_base[ytrue], 1e-8, 1.0))
        ce_mix  = -np.log(np.clip(probs_mix[ytrue], 1e-8, 1.0))
        gain = ce_base - ce_mix   # positive if mix improves NLL

        # --- cost-sensitive misclassification penalty (benign FP >> attack FN) ---
        yhat = int(np.argmax(probs_mix))
        cost = 0.0
        if yhat != ytrue:
            if ytrue == 0 and yhat != 0:
                # Strongly punish false alarms on benign; scale by how confident we were it's not benign
                conf_not_benign = 1.0 - float(probs_mix[0])
                cost += self.fp_cost * (1.0 + 1.5 * conf_not_benign)  # typically > fp_cost
            else:
                # Gentle FN cost using class-aware vector
                cost += float(self.fn_cost_vec[ytrue])



        # --- calibration bonus (lower confidence error is better) ---
        conf = float(np.max(probs_mix))
        correct = 1.0 if yhat==ytrue else 0.0
        calib_err = abs(conf - correct)   # per-sample proxy for ECE
        calib_bonus = -calib_err

        # --- sparsity bonus: prefer metabolically sparse rates (median small; few spikes) ---
        med = float(self.SPM[self._i].mean())       # across bases
        frac3 = float(self.SPF[self._i].mean())
        sparsity_score = -(med + 0.5*frac3)         # lower better → negative is bonus

        # --- OOD penalty from flow log-density (z-scored); enforce diversity on low-density ---
        dens = float(self.DENS[self._i].mean())
        conc = float(np.max(w))
        ood_pen = 0.0
        if dens < self.dens_threshold:
            ood_pen = (self.dens_threshold - dens) * conc  # high conc on low density is bad

        # --- diversity constraint on weights ---
        w_ent = -float(np.sum(w*np.log(np.clip(w,1e-8,1.0))))
        div_pen = max(0.0, self.target_w_entropy - w_ent)  # push entropy to a feasible target

        reward = gain - cost + self.ent_w*w_ent + self.calib_w*calib_bonus + self.sparse_w*sparsity_score - self.ood_w*ood_pen - self.div_w*div_pen
        terminated = True; truncated = False; info = {"w": w, "dens": dens, "gain": gain}
        self._i = (self._i + 1) % self.N
        return self._obs(), reward, terminated, truncated, info

# --- build validation streaming env for PPO ---
env_tr = WeightedStackEnv(P_va_tr, ENT_va_tr, DENS_va_tr, SPM_va_tr, SPF_va_tr, y_va_tr,
                          base_prob_va=BASE_PROB_VA_tr,
                          # === EDIT: make benign FP very costly; base FN cost lighter ===
                          fp_cost=12.0, fn_cost=0.7,
                          # entropy/diversity stronger, calibration smaller
                          ent_w=0.015, div_w=0.05,
                          ood_w=0.10, calib_w=0.00, sparse_w=0.02,
                          dens_threshold=dens_gate,
                          fn_cost_vec=fn_cost_vec)

# env_tr.use_aux = False  # CE-only reward ablation
vec_env = DummyVecEnv([lambda: env_tr])

ppo = PPO(policy="MlpPolicy", env=vec_env, n_steps=4096, batch_size=1024, learning_rate=3e-4, ent_coef=0.01, gae_lambda=0.95, gamma=0.99, clip_range=0.2, verbose=0, seed=42)
print("[PPO] training with uncertainty/density/sparsity-aware rewards ...")
ppo.learn(total_timesteps=120_000)

def ppo_weights(ppo_model, P, ENT, DENS, SPM, SPF):
    N = P.shape[0]; K = P.shape[2]
    W = np.zeros((N, K), dtype=np.float32)
    featurizer = StackStateFeaturizer(n_classes, K)
    for i in range(N):
        s = featurizer.make(P, ENT, DENS, SPM, SPF, i).reshape(1,-1)
        a, _ = ppo_model.predict(s, deterministic=True)
        w_raw = stick_breaking(np.asarray(a).reshape(1,-1))[0]
        # === blend with uniform to avoid collapse (stabilizes benign recall) ===
        lam = 0.7  # 0..1; higher = trust PPO more
        uniform = np.ones(K, dtype=np.float32) / K
        W[i] = (1.0 - lam) * uniform + lam * w_raw
    return W


W_tr = ppo_weights(ppo, P_tr, ENT_tr, DENS_tr, SPM_tr, SPF_tr)
W_va = ppo_weights(ppo, P_va, ENT_va, DENS_va, SPM_va, SPF_va)
W_te = ppo_weights(ppo, P_te, ENT_te, DENS_te, SPM_te, SPF_te)

def mix(P, W):  # [N,C,K] x [N,K] -> [N,C]
    return np.einsum("nck,nk->nc", P, W, dtype=np.float32)

Pmix_tr = mix(P_tr, W_tr); Pmix_va = mix(P_va, W_va); Pmix_te = mix(P_te, W_te)

np.save("artifacts/ppo_weights_train.npy", W_tr)
np.save("artifacts/ppo_weights_val.npy",   W_va)
np.save("artifacts/ppo_weights_test.npy",  W_te)

# ---------- XGBoost as META-LEARNER (final) ----------
# Features: PPO-mixed probs + PPO weights + across-base variance (C) + mean density/entropy (2 scalars)
def meta_features(Pmix, W, P, ENT, DENS):
    varC = P.var(axis=2)                          # [N,C]
    dens_m = DENS.mean(axis=1, keepdims=True)     # [N,1]
    ent_m  = ENT.mean(axis=1, keepdims=True)      # [N,1]
    return np.hstack([Pmix, W, varC, dens_m, ent_m]).astype(np.float32)

X_meta_tr = meta_features(Pmix_tr, W_tr, P_tr, ENT_tr, DENS_tr)
X_meta_va = meta_features(Pmix_va, W_va, P_va, ENT_va, DENS_va)
X_meta_te = meta_features(Pmix_te, W_te, P_te, ENT_te, DENS_te)

meta_xgb = XGBClassifier(
    objective="multi:softprob", eval_metric="mlogloss", num_class=n_classes,
    tree_method="gpu_hist", predictor="gpu_predictor", gpu_id=0,
    random_state=42, n_jobs=0,
    max_depth=7, n_estimators=550, learning_rate=0.055,
    subsample=0.9, colsample_bytree=0.9, max_bin=256,
    min_child_weight=4, reg_alpha=0.0, reg_lambda=1.5, gamma=0.5
)

# --- class-balanced sample weights for meta-XGB (extra push on class 6) ---
counts_meta = np.bincount(y_train, minlength=n_classes).astype(np.float64)
inv_freq = counts_meta.sum() / np.clip(counts_meta, 1.0, None)
inv_freq = inv_freq * (n_classes / inv_freq.sum())  # normalize to ~mean 1
# modest extra emphasis on class 6
inv_freq[6] = inv_freq[6] * 1.25

sw_meta = inv_freq[y_train].astype(np.float32)
# === sample weights — make benign count more at meta stage ===
w_meta = np.ones_like(y_train, dtype=np.float32)
w_meta[y_train == 0] = 1.3   # benign gets 30% more weight
meta_xgb.fit(X_meta_tr, y_train, eval_set=[(X_meta_va, y_val)], sample_weight=w_meta, verbose=False)



probs_te  = meta_xgb.predict_proba(X_meta_te)
y_pred_te = probs_te.argmax(axis=1)

print("\n=== XGBoost META (PPO-weighted generative bases) ===")
print("Accuracy:", round(accuracy_score(y_test, y_pred_te), 4))
print("Log Loss:", round(log_loss(y_test, probs_te), 4))
print("\nClassification Report:\n", classification_report(y_test, y_pred_te, digits=4))

# AUROC (per-class and binary)
y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))
aucs = {i: roc_auc_score(y_test_bin[:, i], probs_te[:, i]) for i in range(n_classes)}
print("Per-class AUROC (meta):", {k: round(v,4) for k,v in aucs.items()})

attack_mask = (y_test != 0).astype(int)
attack_score = 1.0 - probs_te[:, 0]
from sklearn.metrics import precision_recall_curve, roc_curve
fpr, tpr, _ = roc_curve(attack_mask, attack_score)
auc_bin = roc_auc_score(attack_mask, attack_score)
precision, recall, _ = precision_recall_curve(attack_mask, attack_score)
ap_bin = average_precision_score(attack_mask, attack_score)
print(f"Binary AUROC (meta): {auc_bin:.4f} | PR-AUC: {ap_bin:.4f}")

# FPR at fixed TPRs
for target_tpr in [0.90, 0.95, 0.99]:
    idx_best = np.argmin(np.abs(tpr - target_tpr))
    thr = _[idx_best]
    fpr_at = fpr[idx_best]
    print(f"TPR≈{target_tpr:.2f} -> FPR={fpr_at:.4f} (thr={thr:.4f})")





# === Plots: Binary ROC, Binary PR, and Per-class PR curves ===
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve
from sklearn.preprocessing import label_binarize
import os

os.makedirs("artifacts", exist_ok=True)

# --- Binary ROC (Normal vs Attack) ---
plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label=f"Binary AUROC={auc_bin:.4f}")
plt.plot([0,1],[0,1],'--',linewidth=1)
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("Binary ROC (Normal vs Attack) — Meta XGBoost")
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("artifacts/meta_binary_ROC.png", dpi=140)
plt.show()

# --- Binary Precision–Recall ---
plt.figure(figsize=(7,5))
plt.plot(recall, precision, label=f"PR-AUC={ap_bin:.4f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Binary Precision–Recall (Normal vs Attack) — Meta XGBoost")
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("artifacts/meta_binary_PR.png", dpi=140)
plt.show()

# --- Per-class Precision–Recall curves ---
# y_test_bin might already be defined above; if not, re-create it:
try:
    y_test_bin
except NameError:
    y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))

plt.figure(figsize=(9,7))
leg_items = []
for c in range(n_classes):
    prec_c, rec_c, _thr_c = precision_recall_curve(y_test_bin[:, c], probs_te[:, c])
    ap_c = average_precision_score(y_test_bin[:, c], probs_te[:, c])
    plt.plot(rec_c, prec_c, linewidth=1.2, label=f"class {c} AP={ap_c:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Per-class Precision–Recall — Meta XGBoost")
plt.legend(ncol=3, fontsize=8)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig("artifacts/meta_per_class_PR.png", dpi=140)
plt.show()

print("Saved plots to artifacts/: meta_binary_ROC.png, meta_binary_PR.png, meta_per_class_PR.png")

# BLOCK 6 — Latent-channel importance + gradient probe
import os, numpy as np, torch
from collections import OrderedDict
from xgboost import XGBClassifier
import joblib

# -------------------------
# 1) Load latents & labels
# -------------------------
Z_train = np.load("Z_train.npy")
Z_test  = np.load("Z_test.npy")
y_test  = np.load("y_test_sir.npy")
# we also need y_train to (re)fit a quick latent-XGB if not saved
y_train = np.load("y_train_sir.npy")

latent_dim = Z_train.shape[1]

# -------------------------------------------------------
# 2) Get an XGB trained on latents to rank latent channels
#    Try loading a saved model; otherwise fit & save it.
# -------------------------------------------------------
xgb_lat_path = "artifacts/xgb_latents.joblib"
os.makedirs("artifacts", exist_ok=True)

if os.path.exists(xgb_lat_path):
    xgb_lat = joblib.load(xgb_lat_path)
else:
    # quick but decent defaults; GPU if available
    xgb_lat = XGBClassifier(
        objective="multi:softprob",
        eval_metric="mlogloss",
        num_class=int(np.max(y_train) + 1),
        tree_method="gpu_hist" if torch.cuda.is_available() else "hist",
        predictor="gpu_predictor" if torch.cuda.is_available() else "auto",
        gpu_id=0 if torch.cuda.is_available() else -1,
        random_state=42,
        n_jobs=0,
        max_depth=8,
        n_estimators=350,
        learning_rate=0.06,
        subsample=0.9,
        colsample_bytree=0.9,
        max_bin=256
    )
    xgb_lat.fit(Z_train, y_train, verbose=False)
    joblib.dump(xgb_lat, xgb_lat_path)

# guard: some XGB versions expose .feature_importances_ as float32
latent_importance = np.array(xgb_lat.feature_importances_, dtype=float)
latent_importance = latent_importance[:latent_dim]  # clip just in case

# pick top-k channels (keep k <= latent_dim)
k_top = min(16, latent_dim)
top_idx = np.argsort(-latent_importance)[:k_top].astype(np.int64)

print("Top latent channels by XGB importance:")
for i in top_idx:
    print(f"  z[{i:02d}] → {latent_importance[i]:.5f}")

# -------------------------------------------------------
# 3) Prepare L3 numeric view + feature names (for probing)
# -------------------------------------------------------
import pandas as pd

# prefer L3-specific column names if present
num_cols_l3_path = "artifacts/num_cols_l3.pkl"
if os.path.exists(num_cols_l3_path):
    num_cols = list(pd.read_pickle(num_cols_l3_path))
else:
    # fallback: generic names
    fallback_path = "artifacts/num_cols.pkl"
    num_cols = list(pd.read_pickle(fallback_path)) if os.path.exists(fallback_path) else []

# load L3 numeric test (or fallback), and categories
if os.path.exists("Xnum_l3_test_sir.npy"):
    Xnum_test_l3 = np.load("Xnum_l3_test_sir.npy")
else:
    Xnum_test_l3 = np.load("Xnum_test_sir.npy")

Xcat_test = np.load("Xcat_test_sir.npy")

# device + model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
chain.eval()

# infer expected numeric width for L3 encoder: in_features = num_dim + emb_dim
with torch.no_grad():
    in_features = chain.l3.enc[0].in_features
    emb_dim = sum(e.embedding_dim for e in chain.l3.embeddings)
    expected_num_dim = int(in_features - emb_dim)
    if expected_num_dim <= 0:
        raise RuntimeError(f"Unexpected encoder dims: in_features={in_features}, emb_dim={emb_dim}")

# align numeric matrix to expected_num_dim (slice/pad) + align names
cur_dim = Xnum_test_l3.shape[1]
if cur_dim > expected_num_dim:
    Xnum_test_l3_view = Xnum_test_l3[:, :expected_num_dim].copy()
    if len(num_cols) >= cur_dim:
        num_cols = num_cols[:expected_num_dim]
elif cur_dim < expected_num_dim:
    pad = np.zeros((Xnum_test_l3.shape[0], expected_num_dim - cur_dim), dtype=Xnum_test_l3.dtype)
    Xnum_test_l3_view = np.hstack([Xnum_test_l3, pad])
    if len(num_cols) < expected_num_dim:
        extra = [f"pad_{i}" for i in range(expected_num_dim - len(num_cols))]
        num_cols = list(num_cols) + extra
else:
    Xnum_test_l3_view = Xnum_test_l3

# final safety on names length
if len(num_cols) < expected_num_dim:
    num_cols = list(num_cols) + [f"feat_{i}" for i in range(expected_num_dim - len(num_cols))]
else:
    num_cols = num_cols[:expected_num_dim]

# -----------------------------------------
# 4) Gradient probe on top latent channels
# -----------------------------------------
# sample a subset to keep it quick & stable
rng = np.random.default_rng(0)
subsz = min(4096, Xnum_test_l3_view.shape[0])
idx = rng.choice(Xnum_test_l3_view.shape[0], size=subsz, replace=False)

xb_num = torch.from_numpy(Xnum_test_l3_view[idx]).float().to(device)
xb_cat = torch.from_numpy(Xcat_test[idx]).long().to(device)

# require grads on numeric inputs
xb_num.requires_grad_(True)

# convert top indices to tensor (clip to latent_dim)
top_idx = np.asarray(top_idx, dtype=np.int64)
top_idx = top_idx[top_idx < latent_dim]
top_idx_t = torch.as_tensor(top_idx, dtype=torch.long, device=device)

with torch.enable_grad():
    # forward only L3 to match the numeric/cat inputs we prepared
    out3 = chain.l3(xb_num, xb_cat, y=None, deterministic=True)
    # sum the selected latent channels (recalibrated zR)
    zR = out3["zR"]
    # guard if zR has fewer dims than expected
    max_col = zR.size(1)
    if top_idx_t.numel() == 0:
        raise RuntimeError("No valid top latent indices to probe.")
    top_idx_t = top_idx_t[top_idx_t < max_col]
    target = zR.index_select(1, top_idx_t).sum()

    # zero old grads & backprop
    if xb_num.grad is not None:
        xb_num.grad.zero_()
    target.backward()

    # average absolute gradient across the subset
    g = xb_num.grad.detach().abs().mean(dim=0).cpu().numpy()

# rank input features by gradient magnitude
order = np.argsort(g)[::-1][:20]

print("\nTop input features influencing selected z channels (L3 probe):")
for j in order:
    name = num_cols[j] if j < len(num_cols) else f"feat_{j}"
    print(f"  {name}: |grad|={g[j]:.6f}")

